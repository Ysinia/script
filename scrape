import csv
import os
import requests
import gzip
from urllib.parse import urlparse, urljoin
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib import robotparser
import logging

# configure the logger to write logs to a file
logging.basicConfig(filename='scraper.log', level=logging.ERROR)

base_url = "https://www.example.com"

# Une liste d'URLs à scraper
urls = [
    "/article",
    "/page1",
    "/page2",
    "/page3"
]

for url in urls:
    try:
        # Concaténez l'URL avec la base_url
        full_url = base_url + url
        
        # Envoyez une requête GET pour obtenir le contenu HTML de la page
        response = requests.get(full_url)
        
        # Créez une instance BeautifulSoup pour analyser le HTML de la page
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Trouvez la balise h1
        h1_tag = soup.find('h1')
        
        # Si la balise h1 n'est pas trouvée, imprimez un message et continuez avec la prochaine URL
        if h1_tag is None:
            print(f"Aucune balise h1 trouvée sur {full_url}")
            continue
        
        # Imprimez le texte de la balise h1
        print(h1_tag.text)
        
    except requests.exceptions.HTTPError as e:
        # Si une erreur HTTP se produit, imprimez le code d'erreur et continuez avec la prochaine URL
        print(e)
        continue
    
    except Exception as e:
        # Si une autre exception se produit, imprimez le message d'erreur et continuez avec la prochaine URL
        print(f"Erreur lors de la requête sur {full_url}: {e}")
        continue

# créer un ensemble pour stocker les URLs déjà scrapées
urls_scraped = set()

# Ouvrir le fichier CSV contenant la liste des URLs
with open('liste_sites.csv', newline='') as csvfile:
    url_reader = csv.reader(csvfile, delimiter=' ', quotechar='|')
    # Parcourir chaque ligne du fichier CSV et extraire l'URL
    for row in url_reader:
        try:
            url = row[0]
            # Appeler la fonction scrape_url pour chaque URL extraite
            html = scrape_url(url)
            urls_scraped.add(url)
        except Exception as e:
            print(f"Erreur lors du traitement de l'URL {url}: {e}")

def scrape_all_links(url, visited_links=None):
    """
    Scrape tous les liens de la page URL donnée, en appelant la fonction
    scrape_all_links() pour chaque lien non encore visité.

    Args:
        url (str): L'URL de la page à scraper.
        visited_links (set): Ensemble des liens déjà visités.

    Returns:
        visited_links (set): Ensemble des liens visités après le scraping de la page URL.
    """

    if visited_links is None:
        visited_links = set()

    # Vérifie si le lien a déjà été visité
    if url in visited_links:
        return visited_links

    # Ajoute l'URL à la liste des liens visités
    visited_links.add(url)

    # Récupère le contenu HTML de la page
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.error(f"Error scraping {url}: {e}")
        return visited_links

    soup = BeautifulSoup(response.content, 'html.parser')

    # Scrape tous les liens de la page
    for link in soup.find_all('a'):
        link_url = link.get('href')

        # Vérifie si le lien est valide
        if link_url is not None and link_url.startswith('http'):

            # Appelle la fonction scrape_all_links() pour chaque lien non encore visité
            if link_url not in visited_links:
                visited_links = scrape_all_links(link_url, visited_links)

    return visited_links

# Définir la fonction get_urls_from_csv en haut du script
def get_urls_from_csv(filename):
    urls = []
    with open(filename, 'r') as f:
        reader = csv.reader(f)
        for row in reader:
            urls.append(row[0])
    return urls

# Appeler la fonction get_urls_from_csv pour obtenir une liste d'URLs
url_list = get_urls_from_csv('liste_sites.csv')


def is_allowed_by_robots(url):
    rp = robotparser.RobotFileParser()
    rp.set_url(url + '/robots.txt')
    rp.read()
    return rp.can_fetch('*', url)

    # Fonction pour extraire le contenu HTML d'une page
def get_html(url):
    headers = {'Accept-Encoding': 'gzip'}
    response = requests.get(url, headers=headers, stream=True)
    if response.status_code == 200:
        content = b""
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                content += chunk
        try:
            html = gzip.decompress(content).decode("utf-8")
        except (gzip.BadGzipFile, OSError):
            html = ""
        return html
    else:
        print(f"Erreur lors de la requête sur {url} : {response.status_code}")
        return ""

dimport csv

def is_url_scraped(url):
    with open('scraped_urls.csv', 'r') as file:
        reader = csv.reader(file)
        rows = list(reader)
        if not any(rows):
            return False
        for row in rows:
            if row and url == row[0]:
                return True
    return False
# créer un ensemble pour stocker les URLs déjà scrapées
urls_scraped = set()

# appeler la fonction scrape_url avec l'ensemble urls_scraped comme argument
scrape_url("http://www.example.com", urls_scraped)


def parse_page(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    # Vérifier la présence de la balise h1
    h1_tag = soup.find('h1')
    if not h1_tag:
        print("La page ne contient pas de balise h1.")
        return None
    
    # Extraire le texte de la balise h1
    title = h1_tag.text.strip()

    # Extraire les informations
    title = soup.find('h1').text.strip()
    subtitles = [h.text.strip() for h in soup.find_all(['h2', 'h3'])]
    images = [img['src'] for img in soup.find_all('img')]
    content = soup.find('article').text.strip()
    meta_title = soup.find('meta', {'name': 'title'})['content']
    meta_description = soup.find('meta', {'name': 'description'})['content']

    # Compter les mots dans le contenu
    word_count = len(content.split())

    # Filtrer les articles de moins de 700 mots
    if word_count < 700:
        print(f"L'article '{title}' contient moins de 700 mots, il ne sera pas enregistré.")
        return None

    # Filtrer les articles publiés il y a plus d'un an
    date_str = soup.find('meta', attrs={'property': 'article:published_time'})['content']
    pub_date = datetime.fromisoformat(date_str)
    if pub_date < datetime.now() - timedelta(days=365):
        print(f"L'article '{title}' a été publié il y a plus d'un an, il ne sera pas traité.")
        return None

    data = {'title': title,
            'subtitles': subtitles,
            'images': images,
            'content': content,
            'meta_title': meta_title,
            'meta_description': meta_description}

    return data

# Ouvrir le fichier CSV contenant la liste des URLs
with open('liste_sites.csv', newline='') as csvfile:
    url_reader = csv.reader(csvfile, delimiter=' ', quotechar='|')
    # Parcourir chaque ligne du fichier CSV et extraire l'URL
    for row in url_reader:
        try:
            url = row[0]
            # Appeler la fonction scrape_url pour chaque URL extraite
            scrape_url(url)
        except Exception as e:
            print(f"Erreur lors du traitement de l'URL {url}: {e}")

# Définir la fonction scrape_url pour extraire les données d'une URL
def scrape_url(url, urls_scraped):
    # Vérifier si l'URL est autorisée dans le fichier robots.txt
    if not is_allowed_by_robots(url):
        print(f"L'URL {url} n'est pas autorisée dans le fichier robots.txt.")
        return None
    
    # Vérifier si l'URL a déjà été scrapée
    if is_url_scraped(url, urls_scraped):
        print(f"L'URL {url} a déjà été scrapée.")
        return None
    
    # Extraire le contenu HTML de la page
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Impossible de récupérer le HTML de la page {url}.")
        print(e)
        return None
    html = response.content
    
    # Vérifier si la page contient les informations nécessaires
    data = parse_page(html)
    if data is None:
        return None
    
    # Enregistrer les données dans le fichier donnees_scrapes.csv
    today = datetime.now().strftime("%Y-%m-%d")
    write_data(url, data['title'], data['subtitles'], data['images'], data['content'], data['meta_title'], data['meta_description'], today, today)
    
    # Ajouter l'URL à l'ensemble des URLs scrapées
    urls_scraped.add(url)
    
    print(f"L'URL {url} a été scrapée avec succès.")

    # Extraire tous les liens de la page HTML et scraper chaque lien non encore visité
    soup = BeautifulSoup(html, 'html.parser')
    links = soup.find_all('a')
    for link in links:
        href = link.get('href')
        if href:
            # Résoudre les liens relatifs et absolus
            href = urljoin(url, href)
            parsed_href = urlparse(href)
            href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
            
            # Scraper chaque lien non encore visité
            if href not in urls_scraped:
                scrape_url(href, urls_scraped)

# Définir la fonction write_data pour écrire les données dans le fichier donnees_scrapes.csv
def write_data(url, title, subtitles, images, content, meta_title, meta_description, first_scrap_date, last_scrap_date):
    with open('donnees_scrapes.csv', 'a', newline='') as csvfile:
        fieldnames = ['url', 'title', 'subtitles', 'images', 'content', 'meta_title', 'meta_description', 'first_scrap_date', 'last_scrap_date']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not os.path.exists('donnees_scrapes.csv'):
            writer.writeheader()
        writer.writerow({'url': url, 'title': title, 'subtitles': subtitles, 'images': images, 'content': content, 'meta_title': meta_title, 'meta_description': meta_description, 'first_scrap_date': first_scrap_date, 'last_scrap_date': last_scrap_date})

# créer un ensemble pour stocker les URLs déjà scrapées
urls_scraped = set()

def get_publish_date(soup, url):
    # Code pour extraire la date de publication de l'article à partir du contenu HTML
    date_elem = soup.find('span', {'class': 'date'})
    if date_elem:
        date_string = date_elem.text
        date = datetime.strptime(date_string, '%Y-%m-%d')
        one_year_ago = datetime.now() - timedelta(days=365)
        if date > one_year_ago:
            return datetime.now().strftime("%Y-%m-%d")
    else:
        print(f"Aucune date de publication n'a été trouvée pour l'article {url}.")

def is_allowed_by_robots(url):
    rp = robotparser.RobotFileParser()
    rp.set_url(url + '/robots.txt')
    rp.read()
    return rp.can_fetch('*', url)


# Fonction pour vérifier si l'URL est autorisée dans le fichier robots.txt du site
def is_allowed(url):
    rp = robotparser.RobotFileParser()
    rp.set_url(url + '/robots.txt')
    rp.read()
    return rp.can_fetch('*', url)

#Vérifie si le site est autorisé à être scrapé
# Retourne True si oui, False sinon
# Ici, on suppose que tous les sites sont autorisés
def is_authorized(url):
    return True

def get_html(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Referer': 'http://www.example.com',
        'Accept-Language': 'en-US,en;q=0.9',
        'X-Requested-With': 'XMLHttpRequest'
    }
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        content = b""
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                content += chunk

        # Vérifier si la réponse est compressée et décompresser si nécessaire
        if "Content-Encoding" in response.headers:
            if response.headers["Content-Encoding"] == "gzip":
                try:
                    content = gzip.decompress(content)
                except (gzip.BadGzipFile, OSError):
                    pass

        # Vérifier si le type de contenu est HTML
        content_type = response.headers.get("Content-Type", "")
        if "text/html" not in content_type:
            print(f"La réponse pour {url} n'est pas une page HTML.")
            return ""

        # Décoder le contenu et retourner la page HTML
        try:
            html = content.decode("utf-8")
        except UnicodeDecodeError:
            print(f"Impossible de décoder le contenu de la page {url}.")
            return ""

        return html
    else:
        print(f"Erreur lors de la requête sur {url} : {response.status_code}")
        return ""

def get_title(soup):
    # Récupère le titre de la page à partir de son objet BeautifulSoup
    # Retourne le titre si trouvé, None sinon 
    title = soup.find('title')
    if title:
        return title.get_text().strip()
    else:
        return None

def get_subtitles(soup):
    # Récupère les sous-titres de la page à partir de son objet BeautifulSoup 
    # Retourne une liste de sous-titres s'il y en a, une liste vide sinon 
    subtitles = []
    subtitle_tags = soup.find_all(['h2', 'h3'])
    for subtitle_tag in subtitle_tags:
        subtitle_text = subtitle_tag.get_text().strip()
        if subtitle_text:
            subtitles.append(subtitle_text)
    return subtitles

def get_images(soup):
    # Récupère les images de la page à partir de son objet BeautifulSoup 
    # Retourne une liste d'URLs d'images s'il y en a, une liste vide sinon 
    images = []
    image_tags = soup.find_all('img')
    for image_tag in image_tags:
        image_url = image_tag.get('src')
        if image_url:
            images.append(image_url)
    return images

def get_content(soup):
    # Récupère le contenu textuel de la page à partir de son objet BeautifulSoup 
    # Retourne le contenu s'il y en a, None sinon 
    content_tag = soup.find('article')
    if content_tag:
        return content_tag.get_text().strip()
    else:
        return None

def get_publish_date(soup):
    # Recherche la balise "time" contenant la date de publication
    time_tag = soup.find('time', {'datetime': True})
    if time_tag:
        # Récupère la valeur de l'attribut "datetime"
        datetime_str = time_tag.get('datetime')
        # Convertit la chaîne de caractères en objet datetime
        publish_date = datetime.fromisoformat(datetime_str)
        # Formate la date au format "yyyy-mm-dd"
        return publish_date.strftime('%Y-%m-%d')
    else:
        # Si la balise "time" n'est pas trouvée, recherche la balise "meta" contenant la date de publication
        meta_tag = soup.find('meta', {'itemprop': 'datePublished', 'content': True})
        if meta_tag:
            # Récupère la valeur de l'attribut "content"
            datetime_str = meta_tag.get('content')
            # Convertit la chaîne de caractères en objet datetime
            publish_date = datetime.fromisoformat(datetime_str)
            # Formate la date au format "yyyy-mm-dd"
            return publish_date.strftime('%Y-%m-%d')
        else:
            # Si la balise "meta" n'est pas trouvée, retourne None
            return None


def get_meta_title(soup):
    # Récupère le titre de la page tel que défini dans la balise meta "title"
    # Retourne le titre si trouvé, None sinon
    meta_title = soup.find('meta', {'name': 'title'})
    if meta_title:
        return meta_title.get('content').strip()
    else:
        return None

def get_meta_description(soup):
    # Récupère la description de la page telle que définie dans la balise meta "description"
    # Retourne la description si trouvée, None sinon
    meta_description = soup.find('meta', {'name': 'description'})
    if meta_description:
        return meta_description.get('content').strip()
    else:
        return None

def main():
    # URL de la page à scraper
    url = 'https://www.example.com/article'

    # Récupère le HTML de la page
    html = get_html(url)

    if html:
        # Crée un objet BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')

        # Vérifie si le site est autorisé à être scrapé
        if is_allowed(url):
            # Récupère les informations de la page
            title = get_title(soup)
            subtitles = get_subtitles(soup)
            images = get_images(soup)
            content = get_content(soup)
            publish_date = get_publish_date(soup)
            meta_title = get_meta_title(soup)
            meta_description = get_meta_description(soup)

            # Affiche les informations récupérées
            print('Title:', title)
            print('Subtitles:', subtitles)
            print('Images:', images)
            print('Content:', content)
            print('Publish date:', publish_date)
            print('Meta title:', meta_title)
            print('Meta description:', meta_description)
        else:
            print('Ce site n\'est pas autorisé à être scrapé.')
    else:
        print('Impossible de récupérer le HTML de la page.')

if __name__ == '__main__':
    main()
